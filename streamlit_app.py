import streamlit as st
import pandas as pd
import glob
import os
from datetime import datetime, timedelta

st.set_page_config(layout="wide")

PASSWORD = "ycenc1308"

def login():
    st.title("ğŸˆ ì§€ìì²´ í¬ë¡¤ë§ ë¡œê·¸ì¸")
    password = st.text_input("ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”", type="password")
    if st.button("ë¡œê·¸ì¸"):
        if password == PASSWORD:
            st.session_state.logged_in = True
            st.success("ë¡œê·¸ì¸ ì„±ê³µ!")
            st.rerun()
        else:
            st.error("ë¹„ë°€ë²ˆí˜¸ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")

def main_app():
    st.title("ğŸˆ ì§€ìì²´ í¬ë¡¤ë§")
    st.write("2024ë…„ 10ì›” 20ì¼ 22:33 ì—…ë°ì´íŠ¸\n")
    st.write("ì‘ì—…ì§„í–‰ìƒí™© : 125ê°œ site ìµœì‹  1page ìˆ˜ì§‘ ì‘ì—… ì™„ë£Œ\n")
    st.write("í–¥í›„ì§„í–‰ê³„íš : ë‚˜ë¨¸ì§€ site ìµœì‹  í˜ì´ì§€ ìˆ˜ì§‘, ìˆ˜ì§‘ì‹¤íŒ¨ì‚¬ì´íŠ¸ì ê²€, 2pageì´ìƒ ìˆ˜ì§‘í•˜ë„ë¡ ë³€ê²½")
    
    # ì˜¤ëŠ˜ ì¼ì ë° ìµœê·¼ 7ì¼ ê³„ì‚°
    today = datetime.today()
    one_week_ago = today - timedelta(days=7)
    today_str = today.strftime('%Y%m%d')
    
    # ìµœê·¼ íŒŒì¼ 2ê°œë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
    def get_two_recent_files(file_prefix):
        files = glob.glob(f"{file_prefix}_*.xlsx")
        recent_files = []
        for file in files:
            file_date_str = file.split('_')[-1].replace('.xlsx', '')
            try:
                file_date = datetime.strptime(file_date_str, '%Y%m%d')
                if one_week_ago <= file_date <= today:
                    recent_files.append(file)
            except ValueError:
                continue
        if len(recent_files) >= 2:
            return sorted(recent_files, reverse=True)[:2]
        elif len(recent_files) == 1:
            return recent_files[0], None
        return None, None
    
    # ìµœê·¼ ì¼ì£¼ì¼ ë‚´ì˜ íŒŒì¼ì„ ì½ì–´ì˜¤ê¸° ìœ„í•œ í•¨ìˆ˜
    def get_recent_files(file_prefix):
        files = glob.glob(f"{file_prefix}_*.xlsx")
        recent_files = []
        for file in files:
            file_date_str = file.split('_')[-1].replace('.xlsx', '')
            try:
                file_date = datetime.strptime(file_date_str, '%Y%m%d')
                if one_week_ago <= file_date <= today:
                    recent_files.append(file)
            except ValueError:
                continue
        return sorted(recent_files, reverse=True)
    
    # df_log íŒŒì¼ ì½ê¸°
    recent_file_path, previous_file_path = get_two_recent_files('df_log')
    
    # ìµœê·¼ íŒŒì¼ ì²˜ë¦¬
    if recent_file_path:
        df_log_recent = pd.read_excel(recent_file_path, engine='openpyxl')
        st.write(f" - ìµœê·¼ df_log íŒŒì¼: {recent_file_path}ì—ì„œ ë°ì´í„°ë¥¼ ì½ì—ˆìŠµë‹ˆë‹¤.")
    else:
        st.write(" - ìµœê·¼ ì¼ì£¼ì¼ ë‚´ì— df_log íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # ì´ì „ íŒŒì¼ ì²˜ë¦¬
    if previous_file_path:
        df_log_previous = pd.read_excel(previous_file_path, engine='openpyxl')
        st.write(f" - ì´ì „ df_log íŒŒì¼: {previous_file_path}ì—ì„œ ë°ì´í„°ë¥¼ ì½ì—ˆìŠµë‹ˆë‹¤.")
    else:
        st.write(" - ì´ì „ df_log íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # df_log íŒŒì¼ ì²˜ë¦¬
    if recent_file_path and previous_file_path:
        merge_columns = ['URL', 'unique_date', 'max_date']
        df_merged = pd.merge(df_log_recent, 
                             df_log_previous[merge_columns], 
                             on='URL', 
                             suffixes=('_recent', '_previous'), 
                             how='left')
        
        st.write("ìµœê·¼ íŒŒì¼ê³¼ ì´ì „ íŒŒì¼ì„ left joiní•œ ë°ì´í„°:")
        st.dataframe(df_merged, use_container_width=True)
    
        df_merged['max_date_recent'] = pd.to_datetime(df_merged['max_date_recent'], errors='coerce')
        
        today_str = today.strftime('%Y-%m-%d')
        max_date_recent = df_merged['max_date_recent'].max()
        problematic_rows = df_merged[(df_merged['unique_date_recent'].isnull()) | ((df_merged['unique_date_recent'] == 1) & (df_merged['max_date_recent'] == max_date_recent))]
        
        if not problematic_rows.empty:
            st.warning(f"ëœ ìˆ˜ì§‘ëœ ì‚¬ì´íŠ¸ ë¦¬ìŠ¤íŠ¸ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. ì§ì ‘ ì ‘ì† í›„ í™•ì¸ í•„ìš”í•©ë‹ˆë‹¤.")
            st.write("í™•ì¸í•´ì•¼ í•  ì‚¬ì´íŠ¸:")
            st.dataframe(problematic_rows, use_container_width=True)
        else:
            st.success("unique_dateê°€ Nullì´ê±°ë‚˜ 1ì¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.write("ë¹„êµë¥¼ ìœ„í•´ ì´ì „ íŒŒì¼ê³¼ ìµœê·¼ íŒŒì¼ì´ ëª¨ë‘ í•„ìš”í•©ë‹ˆë‹¤.")
    
    # df_list íŒŒì¼ ì½ê¸° ë° ì²˜ë¦¬
    df_list_file_paths = get_recent_files('df_list')
    if df_list_file_paths:
        combined_df_list = pd.DataFrame()
        
        for file_path in df_list_file_paths:
            df = pd.read_excel(file_path, engine='openpyxl')
            combined_df_list = pd.concat([combined_df_list, df], ignore_index=True)
        
        combined_df_list = combined_df_list.drop_duplicates()
    
        column_order = ['SITE_NO', 'ì¶œì²˜', 'ì œëª©', 'URL', 'ì‘ì„±ì¼']
        combined_df_list = combined_df_list.reindex(columns=column_order)
    
        combined_df_list['URL'] = combined_df_list['URL'].apply(lambda x: f'<a href="{x}" target="_blank">{x}</a>')
    
        combined_df_list['ì‘ì„±ì¼'] = pd.to_datetime(combined_df_list['ì‘ì„±ì¼'], errors='coerce')
        combined_df_list = combined_df_list.sort_values(by='ì‘ì„±ì¼', ascending=False)
    
        st.markdown("""
            <style>
            table {
                width: 100%;
            }
            th, td {
                padding: 5px;
            }
            th {
                text-align: left;
            }
            td {
                max-width: 200px;
                overflow-wrap: break-word;
            }
            </style>
        """, unsafe_allow_html=True)
        
        st.write(f"ìµœê·¼ ì¼ì£¼ì¼ ë‚´ì— df_list íŒŒì¼ {len(df_list_file_paths)}ê°œë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
        
        search_keyword = st.text_input("df_list íŒŒì¼ì—ì„œ ê²€ìƒ‰í•  í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”")
    
        if search_keyword:
            search_results = combined_df_list[combined_df_list['ì œëª©'].str.contains(search_keyword, na=False)]
            st.write(f"'{search_keyword}' ê²€ìƒ‰ ê²°ê³¼:")
            st.markdown(search_results.to_html(escape=False), unsafe_allow_html=True)
        else:
            st.write("df_list íŒŒì¼ì˜ ì „ì²´ ë°ì´í„°:")
            st.markdown(combined_df_list.to_html(escape=False), unsafe_allow_html=True)
    else:
        st.write("ìµœê·¼ ì¼ì£¼ì¼ ë‚´ì— df_list íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # ì¤‘ê°„ ì¼ë°°ì¹˜ ìˆ˜ì§‘ ë¡œê·¸ í…ìŠ¤íŠ¸
    st.subheader("ì¼ë°°ì¹˜ ìˆ˜ì§‘ ë¡œê·¸")
    log_text = """
   Processing rows:   0%|          | 0/126 [00:00<?, ?it/s]<ipython-input-1-446351b5a32f>:104: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2023-03-17' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.
  df.at[index, 'min_date'] = min_date
<ipython-input-1-446351b5a32f>:105: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2024-09-03' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.
  df.at[index, 'max_date'] = max_date
Processing rows:   2%|â–         | 2/126 [00:12<14:21,  6.95s/it]ì½ê¸° íƒ€ì„ì•„ì›ƒ: ë¶€ì‚°ì‹œì„¤ê´€ë¦¬ê³µë‹¨ ì„œë²„ê°€ ë°ì´í„°ë¥¼ ì œê³µí•˜ëŠ” ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.
ë¶€ì‚°ì‹œì„¤ê´€ë¦¬ê³µë‹¨í˜ì´ì§€ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
Processing rows:  13%|â–ˆâ–        | 16/126 [01:11<18:31, 10.11s/it]cleaned_dates ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.
Processing rows:  29%|â–ˆâ–ˆâ–‰       | 37/126 [03:26<14:49,  9.99s/it]ê²½ê¸°ë„_ìˆ˜ì›_ì¥ì•ˆêµ¬í˜ì´ì§€ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
Processing rows:  34%|â–ˆâ–ˆâ–ˆâ–      | 43/126 [04:33<10:54,  7.88s/it]ê²½ê¸°ë„_ê´‘ëª…ì‹œí˜ì´ì§€ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
Processing rows:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 47/126 [04:41<04:17,  3.26s/it]ìš”ì²­ ì˜¤ë¥˜: 417 Client Error: Expectation Failed for url: https://www.gimpo.go.kr/portal/ntfcPblancList.do?key=1004&cate_cd=1&searchCnd=40900000000
ê²½ê¸°ë„_ê¹€í¬ì‹œí˜ì´ì§€ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
Processing rows:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 100/126 [12:23<05:46, 13.34s/it]WebDriverException occurred: Message: unknown error: net::ERR_NAME_NOT_RESOLVED
  (Session info: chrome=130.0.6723.58)
Stacktrace:
#0 0x58d94f6d10aa <unknown>
#1 0x58d94f1e81a0 <unknown>
#2 0x58d94f1e03a1 <unknown>
#3 0x58d94f1d0b09 <unknown>
#4 0x58d94f1d283a <unknown>
#5 0x58d94f1d0dbd <unknown>
#6 0x58d94f1d063c <unknown>
#7 0x58d94f1d052d <unknown>
#8 0x58d94f1ce5bc <unknown>
#9 0x58d94f1cebfa <unknown>
#10 0x58d94f1eaa99 <unknown>
#11 0x58d94f2783b5 <unknown>
#12 0x58d94f258d82 <unknown>
#13 0x58d94f277866 <unknown>
#14 0x58d94f258b23 <unknown>
#15 0x58d94f227990 <unknown>
#16 0x58d94f22896e <unknown>
#17 0x58d94f69d16b <unknown>
#18 0x58d94f6a0f68 <unknown>
#19 0x58d94f68a64c <unknown>
#20 0x58d94f6a1ae7 <unknown>
#21 0x58d94f66f4af <unknown>
#22 0x58d94f6bf4f8 <unknown>
#23 0x58d94f6bf6c0 <unknown>
#24 0x58d94f6cff26 <unknown>
#25 0x7fc87774dac3 <unknown>
. Retrying 1/2...
WebDriverException occurred: Message: unknown error: net::ERR_NAME_NOT_RESOLVED
  (Session info: chrome=130.0.6723.58)
Stacktrace:
#0 0x584a8aedf0aa <unknown>
#1 0x584a8a9f61a0 <unknown>
#2 0x584a8a9ee3a1 <unknown>
#3 0x584a8a9deb09 <unknown>
#4 0x584a8a9e083a <unknown>
#5 0x584a8a9dedbd <unknown>
#6 0x584a8a9de63c <unknown>
#7 0x584a8a9de52d <unknown>
#8 0x584a8a9dc5bc <unknown>
#9 0x584a8a9dcbfa <unknown>
#10 0x584a8a9f8a99 <unknown>
#11 0x584a8aa863b5 <unknown>
#12 0x584a8aa66d82 <unknown>
#13 0x584a8aa85866 <unknown>
#14 0x584a8aa66b23 <unknown>
#15 0x584a8aa35990 <unknown>
#16 0x584a8aa3696e <unknown>
#17 0x584a8aeab16b <unknown>
#18 0x584a8aeaef68 <unknown>
#19 0x584a8ae9864c <unknown>
#20 0x584a8aeafae7 <unknown>
#21 0x584a8ae7d4af <unknown>
#22 0x584a8aecd4f8 <unknown>
#23 0x584a8aecd6c0 <unknown>
#24 0x584a8aeddf26 <unknown>
#25 0x7fd8ffe54ac3 <unknown>
. Retrying 2/2...
Processing rows:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 101/126 [13:28<12:02, 28.91s/it]Failed to crawl ì¶©ì²­ë„_ë³´ì€êµ° after 2 retries.
Processing rows:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 104/126 [14:02<06:03, 16.54s/it]ì¶©ì²­ë„_ì¦í‰êµ°í˜ì´ì§€ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
ì¶©ì²­ë„_ì§„ì²œêµ°í˜ì´ì§€ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
Processing rows:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 109/126 [14:25<02:35,  9.16s/it]ì¶©ì²­ë„_ë‹¹ì§„ì‹œí˜ì´ì§€ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
Processing rows:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 112/126 [14:38<01:39,  7.08s/it]ì½ê¸° íƒ€ì„ì•„ì›ƒ: ì¶©ì²­ë„_ì„œì‚°ì‹œ ì„œë²„ê°€ ë°ì´í„°ë¥¼ ì œê³µí•˜ëŠ” ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.
ì¶©ì²­ë„_ì„œì‚°ì‹œí˜ì´ì§€ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
Processing rows:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 114/126 [15:08<02:05, 10.42s/it]WARNING:urllib3.connection:Failed to parse headers (url=https://www.geumsan.go.kr:443/kr/html/sub03/030302.html): [MissingHeaderBodySeparatorDefect()], unparsed data: 'P3P : CP="NOI CURa ADMa DEVa TAIa OUR DELa BUS IND PHY ONL UNI COM NAV INT DEM PRE"\r\nSet-Cookie: SIDNAME=ronty; path=/; HttpOnly; secure; SameSite=None; secure\r\nCache-Control: no-store, no-cache, must-revalidate, post-check=0, pre-check=0\r\nPragma: no-cache\r\nSet-Cookie: PHPSESSID=4aql0a6o580vsmi5j54ojan9n1; path=/; HttpOnly; secure; SameSite=None\r\nExpires: Thu, 19 Nov 1981 08:52:00 GMT\r\n\r\n'
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/urllib3/connection.py", line 510, in getresponse
    assert_header_parsing(httplib_response.msg)
  File "/usr/local/lib/python3.10/dist-packages/urllib3/util/response.py", line 88, in assert_header_parsing
    raise HeaderParsingError(defects=defects, unparsed_data=unparsed_data)
urllib3.exceptions.HeaderParsingError: [MissingHeaderBodySeparatorDefect()], unparsed data: 'P3P : CP="NOI CURa ADMa DEVa TAIa OUR DELa BUS IND PHY ONL UNI COM NAV INT DEM PRE"\r\nSet-Cookie: SIDNAME=ronty; path=/; HttpOnly; secure; SameSite=None; secure\r\nCache-Control: no-store, no-cache, must-revalidate, post-check=0, pre-check=0\r\nPragma: no-cache\r\nSet-Cookie: PHPSESSID=4aql0a6o580vsmi5j54ojan9n1; path=/; HttpOnly; secure; SameSite=None\r\nExpires: Thu, 19 Nov 1981 08:52:00 GMT\r\n\r\n'
Processing rows:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 117/126 [15:30<01:21,  9.07s/it]ì¶©ì²­ë„_ì„œì²œêµ°í˜ì´ì§€ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
Processing rows:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 124/126 [17:12<00:17,  8.92s/it]ìš”ì²­ ì˜¤ë¥˜: 400 Client Error: Bad Request for url: https://www.namwon.go.kr/board/post/list.do?boardUid=ff8080818ea1fec5018ea24137680031&menuUid=ff8080818e3beff0018e4077131b007a&beginDateStr=&endDateStr=&searchType=postTtl&keyword=%EC%A0%9C%EC%95%88&paramString=Us7WVBAxc13kgzv1JU3ayAslPphGSM%2FmlfdB4qtWC4OBeJsElaKmGl7kvQ4Au%2B3O&size=10
ì „ë¼ë„_ë‚¨ì›ì‹œí˜ì´ì§€ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
ì „ë¼ë„_ìµì‚°ì‹œí˜ì´ì§€ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
Processing rows: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126/126 [17:27<00:00,  8.31s/it]ì—°ê²° íƒ€ì„ì•„ì›ƒ: ì „ë¼ë„_ì „ì£¼ì‹œ ì„œë²„ë¡œë¶€í„° ì‘ë‹µì´ ì—†ìŠµë‹ˆë‹¤.
ì „ë¼ë„_ì „ì£¼ì‹œí˜ì´ì§€ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
    """
    st.text(log_text)

# ë©”ì¸ ì‹¤í–‰ íë¦„
if 'logged_in' not in st.session_state:
    st.session_state.logged_in = False

if not st.session_state.logged_in:
    login()
else:
    main_app()
